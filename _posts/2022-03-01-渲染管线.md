<!-- TOC -->

- [游戏编程概述](#游戏编程概述)
    - [一，游戏编程的发展](#一游戏编程的发展)
    - [二，游戏循环](#二游戏循环)
        - [1.传统的游戏循环](#1传统的游戏循环)
        - [2.多线程下的游戏循环](#2多线程下的游戏循环)
    - [三、时间和游戏](#三时间和游戏)
        - [1.通过处理时间增量来表示游戏逻辑](#1通过处理时间增量来表示游戏逻辑)
    - [四、游戏对象](#四游戏对象)
- [图形](#图形)
        - [1.什么是渲染管线](#1什么是渲染管线)
        - [2.渲染管线的流程](#2渲染管线的流程)
            - [2.1总流程图](#21总流程图)
            - [应用阶段](#应用阶段)
            - [几何阶段](#几何阶段)
            - [光栅化阶段](#光栅化阶段)
            - [逐像素阶段](#逐像素阶段)
            - [2.2 抗锯齿处理技术](#22-抗锯齿处理技术)
        - [3.减少DrawCall](#3减少drawcall)
        - [4. Multi Draw Indirect](#4-multi-draw-indirect)
        - [5. Depth Prepass（later z和early z）](#5-depth-prepasslater-z和early-z)
        - [6. alphe测试](#6-alphe测试)
        - [7. Deferred Rendering && Forward Rendering（延迟渲染和前向渲染）](#7-deferred-rendering--forward-rendering延迟渲染和前向渲染)
            - [Forward Rendering](#forward-rendering)
            - [Deferred Rendering](#deferred-rendering)
- [数学](#数学)
    - [坐标系](#坐标系)
        - [2D坐标系](#2d坐标系)
        - [3D坐标系](#3d坐标系)
    - [向量的点乘、叉乘](#向量的点乘叉乘)
        - [点乘：](#点乘)
        - [叉乘](#叉乘)

<!-- /TOC -->
https://zhuanlan.zhihu.com/p/27846162
## 游戏编程概述
### 一，游戏编程的发展
硬件平台的发展，解放生产力，早期平台基本使用汇编语言，调试困难，没有开发工具和SDK（开放代码库），以及大量低质量的游戏涌入市场，导致市场崩溃，大量实体光盘被当作垃圾处理（雅达利大爆炸）。  
进入千禧年后使用高级语言开发，提高开发效率**中间件即开源的出现**（引擎，物理系统）

### 二，游戏循环
整个游戏程序的核心流程控制称为游戏循环。之所以是一个循环，是因为游戏总是不断地执行一系列动作直到玩家退出。每迭代一次游戏循环为1帧。大部分实时游戏每秒钟更新30-60帧。如果一个游戏跑60FPS（帧/秒），那么这个游戏循环每秒要执行60次。

#### 1.传统的游戏循环
一个传统的游戏循环可以分成3部分：处理输入、更新游戏世界、生成输出。一个基本的游戏循环
- input  
会检查各种输入设备，比如键盘、鼠标、手柄。任何外部的输入在这一阶段都要处理完成。网络数据的输入
- update  
会执行所有激活并且需要更新的对象。这可能会有成千上万个游戏对象。
- generate outputs 生成输出  
图形渲染的三个阶段，其他如音频，涵盖了音效、背景音乐、对话，力反馈的输出。对于多人游戏还涉及到

#### 2.多线程下的游戏循环
当我们的画质精致到一定成都时候，我们画面渲染阶段就会十分消耗时间，如果只有一个线程，渲染和更新逻辑都在一个线程时，我们的帧率就会被影响。  

为了完成以上想法，主线程必须处理所有输入、更新游戏世界、处理所有图形以外的输出。它必须提交相关数据给第二条线程，那么第二条线程就可以渲染所有图像。  

但是，当渲染线程绘制的时候，主线程该干什么？我们不想它简单地等着渲染结束，因为这样比单线程还慢。解决的办法是让渲染线程比主线程慢1帧。这个方法的缺点就是会增加输入延迟，玩家的输入要更久才能在画面上有所反馈。假设跳跃键在第2帧就按下。在多线程游戏循环下，输入直到第3帧才开始处理。图形要到第4帧结束才能看到。

### 三、时间和游戏
真实时间 和 游戏时间。比如30FPS(Frame Per Second)的游戏，每帧大约用时33ms。
#### 1.通过处理时间增量来表示游戏逻辑
如果直接使用游戏时间来表示游戏逻辑的话，30FPS和60FPS不同帧率下游戏表现不同，比如移动速度不同，所以我们引入时间增量（deltaTime），即上一帧所运行的真实时间。

    while game is running
        realDeltaTime = time since last frame
        gameDeltaTime = realDeltaTime * gameTimeFactor
    
        //进程输入
        ···
        update game world with gameDeltaTime
    
        //渲染输出
        ···
    loop
现在虽然还可以正常运行，但在物理模拟上还是会有一些问题，所以需要通过锁帧的手段来避免。  
如果出现一些复杂情况，导致某帧比目标帧率的时长长怎么办？为了跟上目标帧率，我们会直接丢弃这一帧，视觉上会有卡顿，所以我们要避免在普通机器上进行过于复杂的渲染。

### 四、游戏对象
- 普通对象 需要绘制也需要更新
- 静态对象 建筑地图等，需要绘制，但不需要更新
- 需要更新但不许需要绘制 摄像机触发器等
- 游戏循环中的对象，class GameObject，通过继承基类或者实现接口

## 图形
图形学的研究内容非常广泛，平时我们开发游戏或者VR应用用到的，属于其中的一个分支：实时渲染（RealtimeRendering）  一，渲染管线

#### 1.什么是渲染管线
渲染管线也称为渲染流水线或像素流水线或像素管线，在某种程度上可以把渲染管线比喻为工厂里面常见的各种**生产流水线**，工厂里的生产流水线是为了提高产品的生产能力和效率，而渲染管线则是**提高显卡的工作能力和效率**。

渲染管线的主要功能就是决定在给定虚拟相机、三维物体、光源、照明模式，以及纹理等诸多条件的情况下，生成或绘制一幅二维图像的过程。

#### 2.渲染管线的流程
##### 2.1总流程图
![image](https://pic3.zhimg.com/80/v2-a1665bf2063e67a0d924e10c2121600e_720w.jpg)

##### 应用阶段

构建图元，对shader进行设置 CPU先对渲染状态进行设置，当准备好后，像缓冲区加入一个命令，DC将这些命令发送给GPU，开始渲染，而一次DC CPU会进行许多工作，比如检测渲染状态。所以要减少DC次数，将模型合并在一起提交   
现在引擎会经过排序，将渲染状态相同 一样的对象连续绘制，所以drawcall并不是限制我们渲染效率的原因，而是Set Pass Call是消耗的重点，它的作用是传递并改变Render State，具体作用就是，把当前需要用到的材质信息，贴图信息，Shader Pass信息等传递到GPU，而Drawcall则是下命令让GPU绘制一堆三角形，至于绘制命令本身实际上并不昂贵。所以可以将同种shader的模型进行合并，并依次drawcall   

如果想减少DC
一般是使用GPU instance 通过指定同一套材质属性，同一套管线状态和同一个模型，在屏幕上同时绘制多次，然和这有一些限制条件，即使贴图可以打包Atlas，材质属性和模型网格可以打包StructuredBuffer，也没法避免每次绘制时顶点数必须一样这一个致命的限制。   

也就是说Draw Call实际上是如下的步骤：

设置渲染状态→绘制图元数据→结束Draw Call，并且设置渲染状态和绘制图元数据的步骤是可以分开的，也就是说对于多个渲染状态相同的模型可以以如下方式在同一个Draw Call里渲染：

设置渲染状态→绘制（图元数据A+图元数据B+图元数据C）→结束Draw Call

这样的话可以把三个Draw Call合成为一个，节省掉很多设置渲染状态的开销。这也就是Unity的合批（Batch）的大致原理。当然合批也有一些限制比如图元数据的顶点数不能超过多少多少，否则会造成渲染状态改变。

##### 几何阶段
![image](https://pic4.zhimg.com/80/v2-872ceab76e6c2030bed2fcce4adc6ad3_720w.jpg)

顶点着色器    
主要就是把顶点坐标从模型空间变换到齐次裁剪空间。

裁剪(culling)   
因为在我们的屏幕上，有很多模型实际上是看不见的（比如被其他模型遮挡住的）。在图形学中，我们的屏幕能看见的东西，被称作摄像机视口。摄像机所能看见的东西是一个视椎体，在这个视椎体之外的东西会被先裁剪掉，有遮挡关系的也会裁剪掉，能看见什么，我们之后处理什么。这样会大大优化性能。  
- 视椎体剔除 应用阶段
- 遮挡剔除  应用阶段
- 视口剔除  几何阶段
- 背面剔除
- 深度剔除  光栅化阶段末期的融合阶段执行，又叫深度检测(或Z缓存检测)。每次将一个图元回执为相应的像素时，都会计算像素位置处图元的深度值，和深度缓存中对应像素的值进行比较，如果新计算出的深度小于缓存中的深度，则更新深度缓存中的值；如果深度值大于深度缓冲中的值，则计算结果被舍弃，深度缓冲的值也无需更新。

再进行视锥剔除的时候我们还要考虑一个问题，提出导致的顶点增加能否避免，由此产生了只剔除完全在三角形之外的手段。并且如果三角形的一个顶点在视口很远的地方那么可能产生整数溢出，导致不在三角形内的像素被光栅化到三角形内产生错误结果，那么我们就给视口设立一个保护带，把超出视口又不会产生整数精度溢出的部分称作保护带，只对既有顶点在视口内，又有顶点在保护带外的三角形做裁剪，这样算是降低了我们裁剪的精细度，但是工作量也减少了，多一些片元交给GPU进行逐像素处理并不会有太大的资源消耗，我偷张图来讲解下这个问题：

![](https://pic2.zhimg.com/80/v2-752e999642780185c99010a754287f35_720w.jpg)



屏幕映射  
屏幕空间是个二维坐标系（平面的，只有x,y），几何图元是三维的（x,y,z）。这一步就是把三维的几何图元一一映射到二维的屏幕空间上,把xy方向映射到屏幕上，z轴存起来留给光栅化阶段用

##### 光栅化阶段
三角形设置  
三个顶点组成一个三角形面片，而这个三角形面片到底在屏幕里占多少像素呢？这就需要知道三角形边缘的表示方式了，所谓的三角形设置，就是输出一个三角形的边的数据，给下一个阶段。  

三角形遍历  
这个阶段是，遍历每一个像素，去检查是否被一个三角网格所覆盖，如果覆盖就会生存一个片元。  这一步会得到很多片元。所谓的片元，类似于像素，但不是像素。片元是一个状态集合，它所包含的信息比像素多很多，比如深度信息法线纹理之类的，都会包含在片元里，算是像素的前身。  

##### 逐像素阶段
经过学习我们在这里可以添加一个阶段就是逐像素阶段
这个阶段可以看作是对我们的像素进行最后输出前的光照计算，所以为了进行进一步的剔除我们在逐像素之前又添加了一个步骤，就是延迟渲染中做的剔除操作

![image](https://pic1.zhimg.com/v2-be584fc85347b7c9c01405d57dab5050_r.jpg)  
一个片元，要经过模板测试，深度测试，最终才能和上一帧的像素进行混合或者覆盖，然后进入颜色缓冲区，变成我们肉眼看到的，屏幕上的像素。一旦有一个片元没有通过测试，那就会被废弃。  通过测试可以根据Alpha进行混合，用来实现半透明的效果，半透明物体的绘制需要严格遵守画家算法由远及近进行绘制，这是因为半透明的不同层级顺序混合出的颜色并不同。

这里还有一个需要注意的知识点：在进行逐像素计算时，并不是就单独传一个片元，但实际上的输入就跟上文中所说的那样是一个由2*2的4个片元组成的正方形。使用这样的正方形的原因在于，可以对每个像素在x轴和y轴上分别找到一个相邻像素计算出屏幕空间中像素之间的差分。这些差分用于在片元着色器中计算贴图的mipmap层级或者filtering（像二次线性过滤、各向异性过滤之类的）。这就可以解决面试中问道的摩尔纹的问题，解决图形边缘的锯齿问题

##### 2.2 抗锯齿处理技术

 **锯齿产生的原因**

光栅化的时候，是以像素中心点是否被三角形覆盖来决定是否生成片段，因此有些片段覆盖了采样点就生成，有些没有覆盖就不生成，最终导致了锯齿现象。

**超采样抗锯齿(SSAA)**

先映射到高分辨率缓存中放大，然后对每个图像像素进行采样，一般取临近2-4个像素，采样混合后，生成最终的像素再缩小还原为原来图像一样的大小

**多重采样抗锯齿(MSAA)**

在光栅化阶段，判断三角形是否被像素覆盖时，会计算多个采样点，然后计算一个覆盖率；在片段着色阶段，每个像素仍然只计算一次颜色值，片段以像素中央来进行计算，只是最后的结果会乘上一个覆盖率；MSAA的高效性在于，他没有每个采样点都计算一次着色，而是每个像素只计算一次着色，最后乘上一个覆盖率；

MSAA的缺点，以及为什么？

[延迟渲染与MSAA的那些事](https://zhuanlan.zhihu.com/p/135444145)

MSAA对延迟渲染的支持不是很好；有三个原因，一个是MSAA本质上是一种发生在光栅化阶段的技术，也就是几何阶段后，着色阶段前，这个技术需要用到场景中的几何信息，但是延迟渲染因为需要节省光照计算的原因，事先把所有信息都放在了GBuffer上，着色计算的时候已经丢失了几何信息；而且关键一点是如果强行这么做，MSAA会增加数倍的带宽性能消耗，因此一般都不会这么做。还有一个原因是是以前DX9的时代，MRT(多重渲染目标)技术不支持MSAA。

![](![preview](https://pic3.zhimg.com/v2-9950f508135bc806c9e1cb4fd688bfb6_r.jpg)

上面知识管线渲染的大致操作，并不是每一套管线都包括这些操作，像是Early z test 只有在延迟渲染的管线里面才会用到。



#### 3.减少DrawCall
通常情况下如果想减少DC有以下方法：

1. 对于使用同材质的临近的Mesh执行合并操作。
2. 收集拥有相同的PipelineState的渲染对象并同时提交。
3. 多线程并行提交。
4. 换更好的硬件

- 第一个方案，毫无疑问可以进垃圾桶了，无论从CPU，还是内存，还是GPU，还是工程的可维护性，这都是个百害无一利的方案。静态合并Mesh增加包体，动态合并Mesh需要调动CopyBufferRegion指令，再加上剔除精度的下降，这些算力和带宽的消耗已经比多的那一点Drawcall高出许多了，就连Unity都宣布Batching已经进垃圾桶了，可见这个方案有多么落后。

- 第二个方案，是目前公认的最通用的方法，Unity的SRP Batcher就是类似的思路，在DirectX12中，有相同的Mesh Layout和相同的Shader Pass的渲染对象都可以使用相同的PipelineState，我们这里的方案是把Shader和MeshLayout这两个整数加起来，作为一个HashMap的Key，就可以在剔除时把相同的对象放置到一起，这会在下方详细介绍。

- 第三个方案是目前主流现代Graphics API支持且推荐的优化方法。先前有被问到过：GPU渲染不都是有严格的先后顺序的吗？为什么可以并行提交？
那么这个问题中，就出现了两处错误。首先，GPU中不同的Drawcall并不是严格先后排序的，比如先提交了A，再提交B，并在A和B之间插入了一个SetRenderTarget，这时有可能是B先往新的Render Target上绘制，而A还在同时的往旧的Render Target上绘制，互不干涉内政，这就是为何要有Resource Barrier和Fence的原因。第二，CPU的并行提交和GPU Queue的执行毫无联系，MSDN的示意图非常清晰的证明了这一点：

![](https://pic1.zhimg.com/80/v2-5c9529e083deae3837f5ccafbb55cc00_720w.jpg)

可以看到，CPU提交是经过CommandList，而GPU执行的顺序是在CommandQueue中的，CPU的提交和GPU的执行甚至都不在同一帧内，更不用说和执行顺序有何关系，至于Queue在提交时是否是纯GPU的工作，是否有后台驱动的线程或者进程介入，这个是厂家的活，我们无从得知，只需要知道ExecuteCommandQueue是一个立即返回的异步操作即可。

通过先前铺垫好的Job System，让剔除和合批工作分到不同的Job内执行，主相机与阴影的剔除合批分别处于不同Job，而Depth Pass，Geometry Pass，Shadowmap Pass，Transparent Pass也分别处于不同Job，中间通过构建依赖保证先后顺序。这样，同时段至少有2个线程，至多有4个线程是在同时执行提交工作的，而这个只是普通渲染物件的提交，与地形，植被等工作线程也同样是并行的，这能够充分利用现代8核心乃至12核心的CPU。

对于古典的CPU管线来说，遮挡剔除比较昂贵，并且我们并不打算在此安排许多的渲染压力，因此目前只考虑视锥剔除。视锥剔除最简单粗暴的方法就是直接遍历每一个物体并计算Bounding Box，然而，这并不是一个很小的开销，我们必须有一个加速结构将这个开销降低。

我们将场景以32米为一个体素块进行切割，当Renderer发生位置移动时，通过Bounding Box判断当前Renderer所处的块，若处于多个块中，则需要让Renderer同时放到多个块中。在剔除时通过BitArray保证不会重复提交。



#### 4. Multi Draw Indirect

传统的绘制是由CPU指定绘制目标，并将数据准备好再通过DrawCall命令发送给GPU、

还有一种方式就是使用Indirect Draw(间接绘制)，不是由CPU指定绘制目标，而是由GPU自行支配，这种时候我们就可以把模型数据长存在显存中，当使用贴图时或者顶点数据，Indirect Draw会提供索引，需要手动在shader中读取显存。

大致过程是，首先将我们需要的绘制信息放到一个结构体中

![image](https://pic2.zhimg.com/v2-f68236f248de386b8238376efb7a0d3d_r.jpg)

![](https://pic2.zhimg.com/80/v2-95a2eca8ec79e8c9953c855ccd6bdcf1_720w.jpg)

这就是我们的数据准备之后就可以创建我们的command命令， 使用CreateCommandSignature完成创建，所做的工作和GPU Instance区别并不大，但是之后的工作是创建一个负责给GPU提供指令的UploadBuffer，这样GPU就会直接从Buffer中读取这些信息，不会再经过CPU端，这样针对cpu再渲染效率上拖累GPU的问题就解决了，除此之外其他不会随着Drawcall改变而改变的数据比如摄像机位置。贴图之类的 ，仍旧按照正常的传递，传递完毕后调用ExecuteIndirect完成本次提交，注意整个pass必须保证Pipeline state统一


#### 5. Depth Prepass（later z和early z）
了解Depth Prepass需要先了解depth test，depth test就是深度测试，是为了解决图形学中遮挡关系而存在的。那么由于远处的物体后渲染，会将之前渲染的物体覆盖住，这无疑是错误的。所以要想渲染正确，就应该解决这种由渲染先后次序导致的问题。对于场景中的每个位置，记录距离相机最近的点到相机之间的距离（深度），渲染下一个物体的每个pixel之前，先比较该pixel的深度与记录下来的深度，如果比记录值大，说明要远，该pixel就不用渲染。 
对于传统的depth test（late z test）是发生在fragment shading之后的。但是仔细分析pipeline的流程后发现其实在光栅化之后fragment shading之前，fragment的深度就可以计算出来了，有了深度其实很多情况下就可以做depth test（early z test）了，这样就可以提前剔除掉被遮挡的fragment，大大节省GPU的计算能力，所以现在的pipeline如下所示（图中省去了tessellation和geometry stage）。  

depth prepass的意义，就是在一个render pass中，首先先渲染物体的深度，然后再渲染该物体的颜色。在渲染深度时，只为了更新depth buffer，所以需要打开depth test以及depth write mask，同时关闭color的write mask；depth buffer更新完成后，渲染该物体的color时，打开depth test并设置比较函数为“Equal”，打开color write mask来着色，由于depth 之前已经更新好了所以这时候应该关闭depth write mask，这样就可以完成该物体的渲染。很好的避免了复杂场景内overdraw 的问题。

later z  和 eraly z

![](https://img-blog.csdnimg.cn/20190705095825369.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2EyNTQyMTI4YQ==,size_16,color_FFFFFF,t_70)

深度测试是为了记录场景内的遮挡信息，如果前面遮挡的物体是半透明的又该如何处理，这就牵扯到alpha 测试.

#### 6. alphe测试  
https://zhuanlan.zhihu.com/p/263566318

首先确定下渲染顺序，一般情况下是先渲染不透明物体，在渲染版透明物体，渲染过程中都要开启depth，但是半透明物体是没有z write的权限的，原因很简单，他并不能遮挡他后面的物体 ，对于半透明物体的渲染，首先通过深度测试确定是否需要culling，如果没有culling，首先z buff肯定什么都不写，而需要在frame buff上，需要根据内存中的color 和 当前frame的color根据alphe进行混合。

补充：early z和alphe测试存在冲突的问题  

提前深度测试可能会与片元着色器中的透明度测试起冲突：

比如在开启了透明测试情况下，一个属于”alphatest“序列的片元已经通过了深度测试（更新了最近新的深度阈值，比它远的片元都被舍弃），但片元进行透明测试时失败，则该片元因此被舍弃，视觉上的结果就是，一个片元明明被舍弃了，却因为在Early-Z步骤中通过了深度测试，导致其背后的物体都被舍弃；此时透过这个本不存在的片元，背后却什么都看不到，但真实的情况是该片元是全透明的，应该可以看得到背后物体才对。换句话说**物体被全透明的物体挡住，这是不可能的**。

因此为了避免这种冲突，**在使用 Early-Z技术时，不可以使用AlphaTest技术。在Unity的实际应用中**，当使用了**AlphaTest，为了避免冲突，Early-Z技术就会失效。**（这也是使用AlphaTest时，场景中因overdraw过多时影响开销的因素）

#### 7. Deferred Rendering && Forward Rendering（延迟渲染和前向渲染）
##### Forward Rendering
前向渲染的一个典型特征是一个几何体（Geometry）从传入显卡进行处理到最后屏幕显示图形的整个过程是不间断的，它是一个线性的处理过程。显卡（GPU）通过将应用程序阶段（CPU）所传递给它的Geometry拆解成顶点，并将其变换（transformed）和分解成为片段（fragment）或者像素，从而进入显示到屏幕前的最后的渲染操作阶段（final rendering treatment）。

![](http://imgtec.eetrend.com/files/2020-04/%E5%8D%9A%E5%AE%A2/100049068-96534-1.jpg)
上图有四个Geometry，对应了4个Drawcall，它们的经过VS->GS->FS，最后到达RenderTarget的阶段的过程是不会被中断的，Render Pipe（渲染管道）一次处理一个Geometry。

这就是我们常说的Forward Rendering，我们一贯的渲染路径都是采取这种方式。

##### Deferred Rendering
延迟二字表明了我们从Geometry到RenderTarget的处理过程不是连续的，它会将所有的Geometry从渲染管道（Render Pipe：VS->GS）输出后（输出到Geometry Buffer，也就是我们常说的G-Buffer），然后通过应用着色（applying shading）得到最后的图像，下图是Deferred Rendering的一般过程：
![](http://imgtec.eetrend.com/files/2020-04/%E5%8D%9A%E5%AE%A2/100049068-96535-2.jpg)
从上图我们可以看出我们避免了对每一个片元再Fragent shader进行光照计算，这样就减少了我们的计算量。
在一个标准的前向渲染管线中，在一个可见场景中，场景中每一个光照（every light）的计算必须针对每一个顶点和每一个片段，我们假设在场景中有100个几何体（Geometries），每一个几何体有1000个顶点，粗略估算场景中会有100,000个多边形，显卡能够比较轻松的处理这样数量的多边形，但是，当这些多边形送入到片段处理器（Fragment shader）时，逐像素的光照计算（per-pixel lighting）将产生极大的开销，the real slowdown（减速） can occur。
并且还会产生overdraw的问题，但是通过延迟渲染技术，再fragment shader进行光照计算之前，使用early - z技术，提前进行depth test 进行片元剔除。

具体的使用方案有**Deferred Shading**：

它实际执行的时第一个pass不会在Vertex Shader和Pixel Shader处理器中进行着色计算（Shading）而是进行深度剔除，着色计算（Shading）会挪到第二个pass中去。在第一个pass中，需要被第二个pass进行着色计算（Shading）的相关信息会被收集起来，比如每一个表面的Positions，Normals和materials信息会通过“render to texture（渲染到纹理）”被渲染到geometry buffer（G-Buffer）。。随后，片段着色器（Fragment Shader）或者像素着色器（Pixel Shader）使用这些屏幕空间（screen space）的纹理缓冲（texture buffers）对每一个像素或者片段进行直接（direct）或者间接（indirect）的光照（lighting）计算。

总结下：延迟渲染就是将物体的几何信息放到Gbuffer中然后在光照处理阶段，使用G-Buffer中的纹理数据，对每个片段进行光照计算，这种渲染方法一个很大的好处就是能保证在G-Buffer中的片段和在屏幕上呈现的像素所包含的片段信息是一样的，因为深度测试已经最终将这里的片段信息作为最顶层的片段。原因当然是我们提前使用了深度测试，使用了early z，也就是说延迟渲染基本思想是，先执行深度测试，再进行着色计算，将本来在物体空间（三维空间）进行光照计算放到了屏幕空间（二维空间）进行处理。

但是也有缺点：a. 内存开销大,读写G-buffers的内存带宽用量是性能瓶颈；b. 对透明的物体的渲染存在问题（不支持混色）；c. 对多重采样抗锯齿(MSAA)处理的支持不友好（不能快速获取顶点信息，因为全部转换未片元了）。





## 数学
### 坐标系
#### 2D坐标系
- 2D 笛卡尔坐标系。主要运用有序数对来表示点的位置信息
- 2D 极坐标：方向和距离来定义点
- 笛卡尔和极坐标之间的转换。

```
  极坐标 to 笛卡尔
  
  x = rcos(θ)
  y = rsin(θ)
  
  笛卡尔 to 极坐标
  r = sqrt(x^2+y^2)
  θ = arctan(y/x)=tan^-1(y/x)  —— tan的反三角函数
  
  warming:当x=0时，正切为无穷大！！！若x=0，则θ=90

```
#### 3D坐标系
- 3D笛卡尔坐标系  
三条轴（有三个相互正交的向量组成）、八个卦限、三个平面（两两组成）
依据新增的Z轴方向的不同，分为左手坐标系（LHS）和右手坐标系（RHS）
- 3D柱面坐标（三维的极坐标）  
只是在二维极坐标上加了一个描述高度的变量（Z轴）
点的表示：(r,θ,h)
- 三维笛卡尔和柱面坐标的相互转换

```
  极坐标 to 笛卡尔
  
  x = rcos(θ)
  y = rsin(θ)
  z = z;
  
  笛卡尔 to 极坐标
  
  r = sqrt(x^2+y^2)
  θ = arctan(y/x)=tan^-1(y/x)  —— tan的反三角函数
  z = z;
  
  warming：x=0时与二维转换的类似，不再复述

```

### 向量的点乘、叉乘
#### 点乘：  
向量的点乘：a · b = |a| * |b| * cosθ  
数学定义：a · b =x1 · x2 + y1 · y2
点乘的几何意义:  
- 计算两个向量之间的夹角,判断这两个向量是否垂直。
- 计算一个向量在另一个向量方向上的投影长度。

#### 叉乘
向量的叉乘：a ∧ b = |a| * |b| * sinθ
数学定义: a X b = x1 · y1 - y2 · x2   
点乘的几何意义:  
- 在三维几何中，向量a和向量b的叉乘结果是一个向量，更为熟知的叫法是法向量，该向量垂直于a和b向量构成的平面。
- 在二维几何中，叉乘还有另外一个几何意义就是：aXb等于由向量a和向量b构成的平行四边形的面积。